"""
NeuroPulse – Batch Mock Prediction Generator

This script generates a small number of **synthetic stroke prediction events** and
injects them directly into the NeuroPulse API via HTTP. It is intentionally simple
and finite (e.g., three cases) so you can:

- Quickly smoke‑test the backend API (`api_server.py`) without Kafka.
- Populate the dashboard with a few realistic‑looking stroke cases.
- Demonstrate AI‑driven stroke triage without starting the full stream processor.

Project / AI context:
- In the full pipeline, the stream processor calls Vertex AI and Gemini, then
  publishes results to the `ai.prediction.output` Kafka topic.
- The API server consumes those predictions and exposes them to the dashboard.
- This script **stands in for that upstream AI pipeline** by synthesizing
  `AiPredictionOutput`‑like payloads and sending them to the
  `POST /api/mock/predictions` endpoint.

The payloads include stroke / LVO probabilities, a derived risk category, routing
recommendations, a high‑level treatment window assessment, and an explanation text
that mimics what an LLM such as Gemini might produce.

All data is **synthetic** and purely for **demo / testing** – it must never be used
for real clinical decision making.
"""

import json
import requests
import time
from datetime import datetime, timezone, timedelta
import random
import uuid

API_BASE_URL = "http://localhost:8000"


def generate_mock_prediction(case_id: str = None, patient_id: str = None) -> dict:
    """
    Generate a single synthetic prediction payload.

    This mirrors the structure of the `AiPredictionOutput` schema so that the
    API server and frontend can treat it the same way as a real AI prediction
    coming from Kafka.

    Args:
        case_id: Optional EMS case / incident identifier; generated if omitted.
        patient_id: Optional synthetic patient identifier; generated if omitted.

    Returns:
        A dictionary containing stroke and LVO probabilities, risk category,
        routing recommendation, time‑window assessment, a list of risk factors,
        and LLM‑style explanation text.
    """
    case_id = case_id or f"CASE-{uuid.uuid4().hex[:8].upper()}"
    patient_id = patient_id or f"PAT-{uuid.uuid4().hex[:8].upper()}"
    
    # Generate realistic-but-random stroke / LVO probabilities that resemble
    # what a Vertex AI model might output.
    stroke_prob = round(random.uniform(0.3, 0.95), 2)
    lvo_prob = round(random.uniform(0.1, 0.7), 2)
    
    # Map raw probabilities into a human‑friendly risk category used throughout
    # the dashboard and documentation.
    if stroke_prob >= 0.8 or lvo_prob >= 0.6:
        risk_category = "CRITICAL"
    elif stroke_prob >= 0.6 or lvo_prob >= 0.4:
        risk_category = "HIGH"
    elif stroke_prob >= 0.3:
        risk_category = "MODERATE"
    else:
        risk_category = "LOW"
    
    # Random minutes since onset to drive treatment window messaging.
    minutes_since_onset = random.randint(15, 180)
    
    # Choose a destination hospital type to exercise routing logic in the UI.
    hospitals = [
        ("HOSP-PRIMARY-01", "PRIMARY_CENTER", 8, 20),
        ("HOSP-COMP-01", "COMPREHENSIVE_CENTER", 15, 10),
    ]
    hospital_id, hospital_type, travel_min, extra_dtn = random.choice(hospitals)
    
    # Coarse treatment window assessment derived from minutes‑since‑onset.
    if minutes_since_onset <= 270:
        time_window = "Within typical IV tPA window."
    elif minutes_since_onset <= 360:
        time_window = "Within extended window for some EVT candidates."
    else:
        time_window = "Outside standard IV tPA window; EVT may still be considered."
    
    # Risk factors represent key clinical findings that contribute to the
    # model's assessment and drive UI explanations and tooltips.
    risk_factors = []
    if random.random() > 0.3:
        risk_factors.append("Face droop")
    if random.random() > 0.3:
        risk_factors.append("Arm weakness")
    if random.random() > 0.3:
        risk_factors.append("Speech abnormality")
    if random.random() > 0.5:
        risk_factors.append(f"Reduced GCS ({random.choice([13, 14])})")
    if random.random() > 0.6:
        risk_factors.append(f"Elevated systolic BP ({random.randint(180, 220)})")
    
    if not risk_factors:
        risk_factors.append("Subtle findings with possible stroke symptoms")
    
    # Generate a short, LLM‑style explanation summary. In production this would
    # be generated by Gemini, but here we use a templated string so the rest of
    # the system can be exercised without external dependencies.
    onset_str = f"{minutes_since_onset} minutes" if minutes_since_onset < 60 else f"{minutes_since_onset // 60} hours"
    
    summary = (
        f"NeuroPulse estimates a {int(stroke_prob * 100)}% probability of acute ischemic stroke "
        f"and a {int(lvo_prob * 100)}% probability of large vessel occlusion in a patient "
        f"with symptoms for approximately {onset_str}. "
        f"Overall risk is categorized as {risk_category}. "
        f"The system recommends routing to {hospital_type.lower().replace('_', ' ')} (ID: {hospital_id})."
    )
    
    actions = [
        "- Maintain airway, breathing, and circulation; avoid hypotension.",
        "- Keep SpO₂ ≥ 94% and manage blood glucose if severely abnormal.",
        "- Perform ongoing neurological reassessment during transport.",
    ]
    
    if hospital_type == "COMPREHENSIVE_CENTER":
        actions.append("- Pre-notify the comprehensive stroke center about suspected LVO for possible EVT.")
    else:
        actions.append("- Pre-notify the primary stroke center for rapid imaging and thrombolysis evaluation.")
    
    if minutes_since_onset > 270:
        actions.append("- Given longer time from onset, emphasize rapid imaging and consider EVT eligibility.")
    
    return {
        "prediction_id": f"PRED-{uuid.uuid4().hex[:10].upper()}",
        "case_id": case_id,
        "patient_id": patient_id,
        "prediction_ts": datetime.now(timezone.utc).isoformat().replace("+00:00", "Z"),
        "model_name": "vertex_ai_stroke_model",
        "model_version": "v1.0",
        "stroke_probability": stroke_prob,
        "lvo_probability": lvo_prob,
        "risk_category": risk_category,
        "recommended_destination_hospital_id": hospital_id,
        "recommended_destination_type": hospital_type,
        "estimated_travel_min_to_recommended": travel_min,
        "estimated_additional_door_to_needle_min_at_recommended": extra_dtn,
        "time_window_assessment": time_window,
        "top_risk_factors": risk_factors,
        "llm_explanation_summary": summary,
        "llm_recommended_actions": "\n".join(actions),
        "llm_model_name": "gemini-1.5-flash",
        "explanation_version": "v1",
        "minutes_since_symptom_onset": minutes_since_onset,
    }


def send_mock_prediction(prediction: dict) -> bool:
    """Send a mock prediction to the API server via POST endpoint."""
    try:
        response = requests.post(
            f"{API_BASE_URL}/api/mock/predictions",
            json=prediction,
            headers={"Content-Type": "application/json"},
            timeout=5
        )
        if response.status_code == 200:
            return True
        else:
            print(f"Error: API returned status {response.status_code}: {response.text}")
            return False
    except requests.exceptions.RequestException as e:
        print(f"Error sending prediction: {e}")
        return False


def main():
    """Generate and display mock predictions."""
    print("Generating mock predictions for local testing...")
    print(f"API Server: {API_BASE_URL}")
    print()
    
    # Check if API is running
    try:
        response = requests.get(f"{API_BASE_URL}/api/health", timeout=2)
        if response.status_code == 200:
            print("✓ API server is running")
        else:
            print("✗ API server returned an error")
            return
    except requests.exceptions.RequestException:
        print("✗ Cannot connect to API server. Make sure it's running on port 8000")
        return
    
    print("\nGenerating 3 mock cases...\n")
    
    # Generate and send multiple cases
    success_count = 0
    for i in range(3):
        case_id = f"CASE-MOCK-{i+1:03d}"
        patient_id = f"PAT-MOCK-{i+1:03d}"
        prediction = generate_mock_prediction(case_id, patient_id)
        
        print(f"Case {i+1}: {case_id}")
        print(f"  Patient: {patient_id}")
        print(f"  Stroke Probability: {int(prediction['stroke_probability'] * 100)}%")
        print(f"  LVO Probability: {int(prediction['lvo_probability'] * 100)}%")
        print(f"  Risk Category: {prediction['risk_category']}")
        
        # Send to API
        if send_mock_prediction(prediction):
            print(f"  ✓ Injected into API server")
            success_count += 1
        else:
            print(f"  ✗ Failed to inject")
        print()
    
    print("\n" + "="*60)
    if success_count > 0:
        print(f"✓ Successfully injected {success_count} mock case(s) into the API server")
        print("  Refresh your dashboard to see the cases!")
    else:
        print("✗ Failed to inject mock cases. Make sure the API server is running.")
    print("="*60)


if __name__ == "__main__":
    main()

